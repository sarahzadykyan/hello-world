{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A-C**\n",
    "\n",
    "This programme is a tokenizer for files with texts written in the English language which excludes email adresses and website adresses. It consists of 3 finctions and some other parts:\n",
    "\n",
    "0) **input** - asks the user of the programme to put the address of the text he wants to be analised in a given input window/box \n",
    "\n",
    "1) **find_mails_sites(k)** - opens the *input file* and finds all email addresses and websites (with the help of *RE*). It puts these 2 types of string data into one list.\n",
    "\n",
    "2) **tokenizer(s)** - is a typical tokinizer which splits all the elements in a text, lowers it all and gives you a list of strings,\n",
    "\n",
    "3) **text_imput(s)** - opens the *input file* and tokenizes it via **tokenizer(s)** and gives you a list of strings.\n",
    "\n",
    "4) **resulttok** - takes the result of **text_imput(s)** and excludes from it the *lowered* results of **find_mails_sites(k)** , gives you a list of strings\n",
    "\n",
    "As a result the file is tokenized \n",
    "\n",
    "**p.s.**: why didn't I want to lower the adresses and mails inside of *find_mails_sites(k) function* (used tokenizer(s) or anything like that, or even why didn't I start searching for them after getting a tokenised file (join and search)? I thought that maybe one day I'll need that list of adresses the way they are (to write letters to all these people) so why not to have a list of proper data.\n",
    "\n",
    "**D**\n",
    "\n",
    "**The program limitations**\n",
    "\n",
    "It doesn't work with links if the link has after, for example \".com\" anything else.\n",
    "\n",
    "**Possible future improvements**:\n",
    "\n",
    "I think of a tokeniser where:\n",
    "\n",
    "1) you can choose out of a list what to exclude out of your text:\n",
    "(a)emails (b)websites (c)links (d)sertain words(list), (e)numerals etc.\n",
    "\n",
    "2) you can choose what to get as a result in addition to your tokenized file:\n",
    "lists of (a), (b), (c), (d), (e) etc \n",
    "\n",
    "*for example:\n",
    "(1) Input: file, \n",
    "(2) I want to exclude (a) and (b) and (c), \n",
    "(3) I want to get separate lists of (a) and (d) and (e), \n",
    "(4) Run*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My text is situated here on my computer or in my folder: prj.txt\n",
      "['i', 'want', 'to', 'be', 'here', 'with', 'you', 'and', 'haha', 'forever']\n"
     ]
    }
   ],
   "source": [
    "Text_file = input(\"My text is situated here on my computer or in my folder: \")\n",
    "#print (Text_file)\n",
    "\n",
    "from string import punctuation\n",
    "def tokenizer(s):\n",
    "    words = s.lower().strip().split() \n",
    "    twords = [word.strip(punctuation) for word in words] \n",
    "    return twords\n",
    "\n",
    "def text_imput(s):\n",
    "    text = open(Text_file).read()\n",
    "    text_tokens=tokenizer(text)\n",
    "\n",
    "    return text_tokens\n",
    "\n",
    "#print (text_imput(Text_file)) \n",
    "\n",
    "def find_mails_sites(k):\n",
    "    k = open(Text_file).read()\n",
    "    pattern1=[]\n",
    "    pattern2=[] \n",
    "    from string import punctuation\n",
    "    import re\n",
    "    pattern1 = re.findall(r'[a-zA-Z)-9.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+', k)\n",
    "    pattern2 = re.findall('(\\\\b(https://\\w+\\.\\w+|http://\\w+\\.\\w+|https://www\\.\\w+\\.\\w|http://www\\.\\w+\\.\\\\w+|www\\.\\w+\\.\\\\w+)+[a-zA_Z.]*)', k)\n",
    "    pattern2new = [item for t in pattern2 for item in t]\n",
    "\n",
    "    mails_sites=pattern1+pattern2new\n",
    "    return mails_sites\n",
    "\n",
    "#print (find_mails_sites(Text_file))\n",
    "\n",
    "resulttok = []\n",
    "tempora_text = text_imput(Text_file) \n",
    "tempora_NB_pre_pre = find_mails_sites(Text_file)\n",
    "tempora_NB_pre = ' '.join(tempora_NB_pre_pre)\n",
    "tempora_NB = tempora_NB_pre.lower().strip().split()\n",
    "resulttok = [each for each in tempora_text if each not in tempora_NB]     \n",
    "print(resulttok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
